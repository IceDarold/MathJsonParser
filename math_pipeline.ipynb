{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Math Task Pipeline\n",
    "\n",
    "This notebook implements a complete, standalone pipeline for processing mathematical tasks. The overall idea is to take a CSV file of natural-language math problems, use a large language model (LLM) to convert them into a structured `MathIR` JSON format, execute the tasks using a symbolic math library, and finally, save the results to an output CSV file.\n",
    "\n",
    "The process is as follows:\n",
    "1. **Configuration**: All settings, such as file paths and model parameters, are defined in one place.\n",
    "2. **LLM JSON Generation**: A `vLLM`-powered generator takes a batch of math problems and converts them into `MathIR` JSON objects.\n",
    "3. **MathIR Execution**: The core logic from `mathir_parser` is embedded to process the `MathIR` objects and compute the answers using `SymPy`.\n",
    "4. **Main Pipeline**: The main function orchestrates the entire workflow, from reading the input CSV to writing the final answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "This cell centralizes all user-configurable parameters. By modifying these variables, you can easily change the input/output files, the language model, and other settings without altering the core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y torch\n",
    "# # If using pip\n",
    "!pip install vllm==0.4.2\n",
    "!pip install grpcio==1.62.2\n",
    "!pip install antlr4-python3-runtime==4.11.0\n",
    "!pip install networkx shapely sage matplotlib gmpy2 scipy numpy sympy mpmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# File Paths\n",
    "INPUT_CSV_PATH = \"/kaggle/input/aic-2025-math/test_public_w_answers.csv\"\n",
    "OUTPUT_CSV_PATH = \"output_answers.csv\"\n",
    "\n",
    "# vLLM and Model Configuration\n",
    "VLLM_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "VLLM_TENSOR_PARALLEL_SIZE = 1\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL = \"INFO\"\n",
    "\n",
    "# Prompt Engineering\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert in mathematics and possessing profound knowledge of the MathIR specification. Your main task is to receive a mathematical problem description in natural language and translate it into a valid JSON object that strictly adheres to the MathIR specification.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup\n",
    "\n",
    "This cell imports all the necessary libraries for the notebook. These include standard libraries for data handling (`pandas`), symbolic mathematics (`sympy`), and LLM interaction (`vllm`), as well as Python's built-in modules for asynchronous operations and data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List, Literal, Optional, Dict, Any, Tuple, Union\n",
    "from pydantic import BaseModel\n",
    "from dataclasses import dataclass\n",
    "import sympy as sp\n",
    "from sympy.parsing.latex import parse_latex\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MathIR Parser and Execution Logic\n",
    "\n",
    "This cell contains the complete `mathir_parser` module, embedded directly into the notebook to ensure it is self-contained. The code defines the `MathIR` data structures using `Pydantic` models, which guarantees that the JSON from the LLM conforms to a strict schema. It also includes the `run_mathir` function, which serves as the execution engine, parsing the `MathIR` object and using `SymPy` to perform the specified mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helpers: LaTeX â†’ SymPy ===\n",
    "def to_sympy_expr(s: str) -> sp.Expr:\n",
    "    \"\"\"Parse a LaTeX string to a SymPy expression without context.\"\"\"\n",
    "    return parse_latex(s)\n",
    "\n",
    "# === Core IR ===\n",
    "class SymbolSpec(BaseModel):\n",
    "    name: str\n",
    "    domain: Literal['R','R+','Z','N','N+','C'] = 'R'\n",
    "\n",
    "class FunctionDef(BaseModel):\n",
    "    name: str\n",
    "    args: List[str]\n",
    "    expr: str\n",
    "\n",
    "class SequenceDef(BaseModel):\n",
    "    name: str\n",
    "    args: List[str]\n",
    "    expr: str\n",
    "\n",
    "class MatrixDef(BaseModel):\n",
    "    name: str\n",
    "    rows: int\n",
    "    cols: int\n",
    "    data: List[List[str]]\n",
    "\n",
    "class DistributionDef(BaseModel):\n",
    "    name: str\n",
    "    kind: Literal['bernoulli','binomial','geometric','poisson','hypergeom']\n",
    "    params: Dict[str, str]\n",
    "\n",
    "class GeometryDef(BaseModel):\n",
    "    id: str\n",
    "    kind: Literal['line','circle','parabola','ellipse','polyline']\n",
    "    equation: Optional[str] = None\n",
    "    params: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class Definitions(BaseModel):\n",
    "    functions: List[FunctionDef] = []\n",
    "    sequences: List[SequenceDef] = []\n",
    "    matrices: List[MatrixDef] = []\n",
    "    distributions: List[DistributionDef] = []\n",
    "    geometry: List[GeometryDef] = []\n",
    "\n",
    "class TransformSpec(BaseModel):\n",
    "    target: str\n",
    "    type: Literal['rotation','translation','scaling','substitution']\n",
    "    angle_deg: Optional[float] = None\n",
    "    center: Optional[List[float]] = None\n",
    "    vector: Optional[List[float]] = None\n",
    "    factor: Optional[float] = None\n",
    "    subs: Optional[Dict[str, str]] = None\n",
    "\n",
    "class Condition(BaseModel):\n",
    "    type: str\n",
    "    expr: Optional[str] = None\n",
    "    object: Optional[str] = None\n",
    "    point: Optional[List[float]] = None\n",
    "    value: Optional[int] = None\n",
    "\n",
    "class TargetIntegral(BaseModel):\n",
    "    type: Literal['integral_def']\n",
    "    expr: str\n",
    "    var: str\n",
    "    limits: List[Any]\n",
    "    name: Optional[str] = None\n",
    "\n",
    "class TargetLimit(BaseModel):\n",
    "    type: Literal['limit']\n",
    "    expr: str\n",
    "    var: str\n",
    "    to: str\n",
    "\n",
    "class TargetSum(BaseModel):\n",
    "    type: Literal['sum']\n",
    "    term: str\n",
    "    idx: str\n",
    "    start: str\n",
    "    end: str\n",
    "\n",
    "class TargetSolve(BaseModel):\n",
    "    type: Literal['solve_for']\n",
    "    unknowns: List[str]\n",
    "    equations: List[str]\n",
    "\n",
    "class TargetIneq(BaseModel):\n",
    "    type: Literal['inequalities']\n",
    "    inequalities: List[str]\n",
    "\n",
    "class TargetMatrixSolve(BaseModel):\n",
    "    type: Literal['solve_for_matrix']\n",
    "    unknown: str\n",
    "\n",
    "class TargetProbability(BaseModel):\n",
    "    type: Literal['probability']\n",
    "    event_expr: str\n",
    "\n",
    "class TargetValue(BaseModel):\n",
    "    type: Literal['value']\n",
    "    name: str\n",
    "    expr: str\n",
    "\n",
    "Target = Union[TargetIntegral, TargetLimit, TargetSum, TargetSolve, TargetIneq, TargetMatrixSolve, TargetProbability, TargetValue]\n",
    "\n",
    "class OutputSpec(BaseModel):\n",
    "    mode: Literal['exact','decimal'] = 'decimal'\n",
    "    round_to: Optional[int] = 3\n",
    "    simplify: bool = True\n",
    "    rationalize: bool = False\n",
    "\n",
    "class ValidationSpec(BaseModel):\n",
    "    tolerance_abs: float = 1e-9\n",
    "    check_domain_violations: bool = True\n",
    "\n",
    "class MathIR(BaseModel):\n",
    "    meta: Dict[str, Any] = {}\n",
    "    task_type: Literal['auto','integral','limit','sum','algebra','matrix','probability','geometry','optimize'] = 'auto'\n",
    "    expr_format: Literal['latex','sympy','infix'] = 'latex'\n",
    "    assumptions: Dict[str, Any] = {}\n",
    "    constants: Dict[str, str] = {}\n",
    "    symbols: List[SymbolSpec] = []\n",
    "    definitions: Definitions = Definitions()\n",
    "    transforms: List[TransformSpec] = []\n",
    "    conditions: List[Condition] = []\n",
    "    targets: List[Target]\n",
    "    output: OutputSpec = OutputSpec()\n",
    "    validation: ValidationSpec = ValidationSpec()\n",
    "\n",
    "# === Runtime context ===\n",
    "@dataclass\n",
    "class Runtime:\n",
    "    symtab: Dict[str, sp.Symbol]\n",
    "    funcs: Dict[sp.Function, sp.Lambda]\n",
    "    sequences: Dict[sp.Function, sp.Lambda]\n",
    "    matrices: Dict[str, sp.Matrix]\n",
    "    distributions: Dict[str, Any]\n",
    "    geometry: Dict[str, Any]\n",
    "    context: Dict[Any, Any]\n",
    "\n",
    "# === Builders ===\n",
    "DOMAIN_MAP = {\n",
    "    'R': sp.S.Reals,\n",
    "    'R+': sp.Interval.Ropen(0, sp.oo),\n",
    "    'Z': sp.S.Integers,\n",
    "    'N': sp.S.Naturals0,\n",
    "    'N+': sp.S.Naturals,\n",
    "    'C': sp.S.Complexes,\n",
    "}\n",
    "\n",
    "def build_runtime(ir: MathIR) -> Runtime:\n",
    "    symtab: Dict[str, sp.Symbol] = {}\n",
    "    for s in ir.symbols:\n",
    "        if s.domain == 'R':\n",
    "            symtab[s.name] = sp.Symbol(s.name, real=True)\n",
    "        elif s.domain == 'Z':\n",
    "            symtab[s.name] = sp.Symbol(s.name, integer=True)\n",
    "        elif s.domain == 'N':\n",
    "            symtab[s.name] = sp.Symbol(s.name, integer=True, nonnegative=True)\n",
    "        elif s.domain == 'N+':\n",
    "            symtab[s.name] = sp.Symbol(s.name, integer=True, positive=True)\n",
    "        elif s.domain == 'C':\n",
    "            symtab[s.name] = sp.Symbol(s.name, complex=True)\n",
    "        else:\n",
    "            symtab[s.name] = sp.Symbol(s.name, real=True)\n",
    "    \n",
    "    context: Dict[str, Any] = {**symtab}\n",
    "    context['pi'] = sp.pi\n",
    "    context['e'] = sp.E\n",
    "    context['i'] = sp.I\n",
    "\n",
    "    funcs: Dict[sp.Function, sp.Lambda] = {}\n",
    "    for f_def in ir.definitions.functions:\n",
    "        func_symbol = sp.Function(f_def.name)\n",
    "        args = [sp.Symbol(a) for a in f_def.args]\n",
    "        func_arg_context = {a.name: a for a in args}\n",
    "        expr_template = to_sympy_expr(f_def.expr)\n",
    "        expr = expr_template.subs(func_arg_context)\n",
    "        funcs[func_symbol] = sp.Lambda(tuple(args), expr)\n",
    "\n",
    "    sequences: Dict[sp.Function, sp.Lambda] = {}\n",
    "    for s_def in ir.definitions.sequences:\n",
    "        seq_symbol = sp.Function(s_def.name)\n",
    "        args = [sp.Symbol(a) for a in s_def.args]\n",
    "        seq_arg_context = {a.name: a for a in args}\n",
    "        expr_template = to_sympy_expr(s_def.expr)\n",
    "        expr = expr_template.subs(seq_arg_context)\n",
    "        sequences[seq_symbol] = sp.Lambda(tuple(args), expr)\n",
    "        \n",
    "    matrices: Dict[str, sp.Matrix] = {}\n",
    "    for m in ir.definitions.matrices:\n",
    "        mat = sp.Matrix([[to_sympy_expr(v).subs(context) for v in row] for row in m.data])\n",
    "        matrices[m.name] = mat\n",
    "\n",
    "    context.update(funcs.items())\n",
    "    context.update(sequences.items())\n",
    "    context.update(matrices)\n",
    "\n",
    "    return Runtime(symtab, funcs, sequences, matrices, {}, {}, context)\n",
    "\n",
    "# === Node executors ===\n",
    "def exec_integral(rt: Runtime, t: TargetIntegral) -> Tuple[str, sp.Expr]:\n",
    "    integration_var = sp.Symbol(t.var, real=True)\n",
    "    a_template = to_sympy_expr(str(t.limits[0]))\n",
    "    b_template = to_sympy_expr(str(t.limits[1]))\n",
    "    expr_template = to_sympy_expr(t.expr)\n",
    "    subs_dict = rt.context.copy()\n",
    "    a = a_template.subs(subs_dict)\n",
    "    b = b_template.subs(subs_dict)\n",
    "    expr = expr_template.subs(subs_dict)\n",
    "    val = sp.integrate(expr, (integration_var, a, b))\n",
    "    return (t.name or 'I', val.doit())\n",
    "\n",
    "def exec_value(rt: Runtime, t: TargetValue, store: Dict[str, sp.Expr]) -> Tuple[str, sp.Expr]:\n",
    "    if '\\\\' in t.expr:\n",
    "        expr_template = to_sympy_expr(t.expr)\n",
    "        subs_dict = {k: v for k, v in {**rt.context, **store}.items()}\n",
    "        expr = expr_template.subs(subs_dict).doit()\n",
    "    else:\n",
    "        local_dict = {}\n",
    "        for k, v in rt.context.items():\n",
    "            if isinstance(k, str):\n",
    "                local_dict[k] = v\n",
    "            elif hasattr(k, 'name'):\n",
    "                local_dict[k.name] = v\n",
    "        for k in store:\n",
    "            if k not in local_dict:\n",
    "                local_dict[k] = sp.Symbol(k)\n",
    "        expr_template = sp.parse_expr(t.expr, local_dict=local_dict)\n",
    "        expr = expr_template.subs(store).doit()\n",
    "    return (t.name, expr)\n",
    "\n",
    "def exec_limit(rt: Runtime, t: TargetLimit) -> Tuple[str, sp.Expr]:\n",
    "    limit_var = sp.Symbol(t.var, real=True)\n",
    "    subs_dict = rt.context.copy()\n",
    "    expr_template = to_sympy_expr(t.expr)\n",
    "    expr = expr_template.subs(subs_dict)\n",
    "    if t.to == \"oo\":\n",
    "        to = sp.oo\n",
    "    elif t.to == \"-oo\":\n",
    "        to = -sp.oo\n",
    "    else:\n",
    "        to_template = to_sympy_expr(t.to)\n",
    "        to = to_template.subs(subs_dict)\n",
    "    val = sp.limit(expr, limit_var, to)\n",
    "    simplified = sp.simplify(val.doit())\n",
    "    return ('limit', simplified)\n",
    "\n",
    "def exec_sum(rt: Runtime, t: TargetSum) -> Tuple[str, sp.Expr]:\n",
    "    import uuid\n",
    "    unique_idx_name = f\"idx_{uuid.uuid4().hex[:8]}\"\n",
    "    idx_var = sp.Symbol(unique_idx_name, integer=True)\n",
    "    if '\\\\' in t.term:\n",
    "        infix = t.term.replace('\\\\frac{', '(').replace('}{', ')/(').replace('}', ')').replace('^', '**')\n",
    "        term_template = sp.parse_expr(infix, local_dict={t.idx: rt.context[t.idx]})\n",
    "    else:\n",
    "        local_dict = {t.idx: rt.context[t.idx]}\n",
    "        for k, v in rt.context.items():\n",
    "            if isinstance(k, str):\n",
    "                local_dict[k] = v\n",
    "            elif hasattr(k, 'name'):\n",
    "                local_dict[k.name] = v\n",
    "        term_template = sp.parse_expr(t.term, local_dict=local_dict)\n",
    "    term_for_sum = term_template.subs({rt.context[t.idx]: idx_var})\n",
    "    subs_dict = {k: v for k, v in rt.context.items() if k != t.idx}\n",
    "    term = term_for_sum.subs(subs_dict).doit()\n",
    "    start = sp.Integer(t.start)\n",
    "    end = sp.Integer(t.end)\n",
    "    if str(end) == 'oo':\n",
    "        end = sp.oo\n",
    "    val = sp.summation(term, (idx_var, start, end))\n",
    "    return ('sum', val.doit())\n",
    "\n",
    "def exec_solve(rt: Runtime, t: TargetSolve) -> Tuple[str, Any]:\n",
    "    unknowns = [rt.context[u] for u in t.unknowns if u in rt.context]\n",
    "    subs_dict = rt.context.copy()\n",
    "    eqs = []\n",
    "    for eq_str in t.equations:\n",
    "        if '=' in eq_str:\n",
    "            lhs, rhs = eq_str.split('=', 1)\n",
    "            lhs_expr = to_sympy_expr(lhs).subs(subs_dict)\n",
    "            rhs_expr = to_sympy_expr(rhs).subs(subs_dict)\n",
    "            eqs.append(sp.Eq(lhs_expr, rhs_expr))\n",
    "        else:\n",
    "            expr = to_sympy_expr(eq_str).subs(subs_dict)\n",
    "            eqs.append(sp.Eq(expr, 0))\n",
    "    solution = sp.solve(eqs, unknowns)\n",
    "    if isinstance(solution, dict):\n",
    "        solution = [solution]\n",
    "    elif isinstance(solution, list):\n",
    "        if solution and isinstance(solution[0], dict):\n",
    "            pass\n",
    "        else:\n",
    "            solution = [dict(zip(unknowns, val if isinstance(val, tuple) else (val,))) for val in solution]\n",
    "    return ('solve', solution)\n",
    "\n",
    "def exec_ineq(rt: Runtime, t: TargetIneq) -> Tuple[str, Any]:\n",
    "    subs_dict = rt.context.copy()\n",
    "    all_symbols = set()\n",
    "    parsed_ineqs = []\n",
    "    for ineq_str in t.inequalities:\n",
    "        ineq_expr = to_sympy_expr(ineq_str).subs(subs_dict)\n",
    "        all_symbols.update(ineq_expr.free_symbols)\n",
    "        parsed_ineqs.append(ineq_expr)\n",
    "    symbols_list = list(all_symbols)\n",
    "    solution = sp.reduce_inequalities(parsed_ineqs, symbols_list)\n",
    "    return ('inequalities', solution)\n",
    "\n",
    "def parse_matrix_expr(expr_str: str, matrices: Dict[str, sp.Matrix | sp.MatrixSymbol]) -> sp.Matrix | sp.MatrixSymbol:\n",
    "    parts = expr_str.replace(' ', '').split('*')\n",
    "    result = None\n",
    "    for part in parts:\n",
    "        is_transpose = part.endswith('.T')\n",
    "        matrix_name = part.replace('.T', '')\n",
    "        if matrix_name not in matrices:\n",
    "            raise ValueError(f\"Matrix '{matrix_name}' not defined\")\n",
    "        matrix = matrices[matrix_name]\n",
    "        if is_transpose:\n",
    "            matrix = matrix.T\n",
    "        if result is None:\n",
    "            result = matrix\n",
    "        else:\n",
    "            result = result * matrix\n",
    "    return result\n",
    "\n",
    "def exec_matrix(rt: Runtime, t: TargetMatrixSolve, ir: MathIR) -> Tuple[str, Any]:\n",
    "    eq = None\n",
    "    for c in ir.conditions:\n",
    "        if c.type == 'matrix_equation' and c.expr:\n",
    "            eq = c.expr\n",
    "            break\n",
    "    if not eq:\n",
    "        return ('matrix', {'error': 'no_matrix_equation'})\n",
    "    try:\n",
    "        left, right = map(str.strip, eq.split('='))\n",
    "        R = parse_matrix_expr(right, rt.matrices)\n",
    "        if '*' in left and t.unknown in left:\n",
    "            parts = left.split('*')\n",
    "            if len(parts) == 2 and parts[1] == t.unknown:\n",
    "                coeff_name = parts[0]\n",
    "                if coeff_name in rt.matrices:\n",
    "                    A = rt.matrices[coeff_name]\n",
    "                    try:\n",
    "                        X = A.inv() * R\n",
    "                        return ('matrix', X)\n",
    "                    except Exception as e:\n",
    "                        return ('matrix', {'error': f'Matrix is not invertible: {e}'})\n",
    "                else:\n",
    "                    return ('matrix', {'error': f'Coefficient matrix {coeff_name} not found'})\n",
    "            else:\n",
    "                return ('matrix', {'error': 'Unsupported matrix equation format'})\n",
    "        else:\n",
    "            return ('matrix', {'error': 'Unsupported matrix equation format'})\n",
    "    except Exception as e:\n",
    "        return ('matrix', {'error': f\"Failed to solve matrix equation: {e}\"})\n",
    "\n",
    "# === Main runner ===\n",
    "def run_mathir(ir: MathIR) -> Dict[str, Any]:\n",
    "    rt = build_runtime(ir)\n",
    "    store: Dict[str, sp.Expr] = {}\n",
    "    results: Dict[str, Any] = {}\n",
    "    for tgt in ir.targets:\n",
    "        k: Optional[str] = None\n",
    "        v: Any = None\n",
    "        try:\n",
    "            if tgt.type == 'integral_def':\n",
    "                k, v = exec_integral(rt, tgt)\n",
    "                if k: store[k] = v\n",
    "            elif tgt.type == 'value':\n",
    "                k, v = exec_value(rt, tgt, store)\n",
    "            elif tgt.type == 'limit':\n",
    "                k, v = exec_limit(rt, tgt)\n",
    "            elif tgt.type == 'sum':\n",
    "                k, v = exec_sum(rt, tgt)\n",
    "            elif tgt.type == 'solve_for':\n",
    "                k, v = exec_solve(rt, tgt)\n",
    "            elif tgt.type == 'inequalities':\n",
    "                k, v = exec_ineq(rt, tgt)\n",
    "            elif tgt.type == 'solve_for_matrix':\n",
    "                k, v = exec_matrix(rt, tgt, ir)\n",
    "            else:\n",
    "                results[f\"{tgt.type}\"] = {\"error\": \"unsupported_target\"}\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing target {tgt.type}: {e}\")\n",
    "            results[f\"{getattr(tgt, 'name', tgt.type)}\"] = {\"error\": str(e)}\n",
    "            continue\n",
    "        \n",
    "        if k is None: continue\n",
    "\n",
    "        if isinstance(v, sp.Expr):\n",
    "            if ir.output.simplify:\n",
    "                v = sp.simplify(v)\n",
    "            if ir.output.mode == 'decimal':\n",
    "                try:\n",
    "                    numerical_value = sp.N(v)\n",
    "                    if ir.output.round_to is not None:\n",
    "                        v = round(numerical_value, ir.output.round_to)\n",
    "                    else:\n",
    "                        v = numerical_value\n",
    "                except (TypeError, ValueError):\n",
    "                    logging.warning(f\"Could not convert expression '{v}' to a decimal value.\")\n",
    "        \n",
    "        results[k] = v\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM JSON Generation\n",
    "\n",
    "The `LLMJsonGenerator` class is a wrapper around `vLLM` that's specifically designed for this pipeline. It's responsible for taking a batch of natural-language tasks, wrapping them in a system prompt, and sending them to the LLM for processing. The `generate_batch` method is optimized to handle multiple tasks in a single call, which significantly speeds up processing. The class also includes error handling to gracefully manage failed LLM calls or invalid JSON responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(level=LOG_LEVEL):\n",
    "    logging.basicConfig(level=level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class LLMJsonGenerator:\n",
    "    def __init__(self, model_name, tensor_parallel_size, system_prompt):\n",
    "        self.llm = LLM(model=model_name, tensor_parallel_size=tensor_parallel_size)\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def generate_batch(self, user_tasks: List[str]) -> List[Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Generates JSON objects from a batch of natural language math tasks.\"\"\"\n",
    "        prompts = [f\"{self.system_prompt}\\n\\nUser Task: {task}\\n\\nJSON Output:\" for task in user_tasks]\n",
    "        sampling_params = SamplingParams(temperature=0.0, max_tokens=2048)\n",
    "        \n",
    "        try:\n",
    "            outputs = self.llm.generate(prompts, sampling_params)\n",
    "            json_results = []\n",
    "            for output in outputs:\n",
    "                response_text = output.outputs[0].text\n",
    "                json_start = response_text.find('{')\n",
    "                json_end = response_text.rfind('}') + 1\n",
    "                if json_start != -1 and json_end != -1:\n",
    "                    json_str = response_text[json_start:json_end]\n",
    "                    try:\n",
    "                        json_results.append(json.loads(json_str))\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.error(f\"Could not parse JSON from LLM response: {json_str}\")\n",
    "                        json_results.append(None)\n",
    "                else:\n",
    "                    logging.error(\"Could not find JSON object in LLM response.\")\n",
    "                    json_results.append(None)\n",
    "            return json_results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during batch LLM generation or JSON parsing: {e}\")\n",
    "            return [None] * len(user_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Processing Pipeline\n",
    "\n",
    "This is the main execution block of the notebook. It orchestrates the entire process:\n",
    "1. It initializes the `LLMJsonGenerator`.\n",
    "2. It loads the math problems from the input CSV file into a pandas DataFrame.\n",
    "3. It calls `generate_batch` to convert all tasks into `MathIR` JSONs in a single, efficient batch operation.\n",
    "4. It then iterates through the results, validates each `MathIR` JSON, and passes it to the `run_mathir` function for computation.\n",
    "5. Finally, it collects the answers, handles any errors, and saves the results to the output CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    setup_logging()\n",
    "    \n",
    "    llm_generator = LLMJsonGenerator(\n",
    "        model_name=VLLM_MODEL_NAME,\n",
    "        tensor_parallel_size=VLLM_TENSOR_PARALLEL_SIZE,\n",
    "        system_prompt=SYSTEM_PROMPT\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        input_df = pd.read_csv(INPUT_CSV_PATH)\n",
    "        logging.info(f\"Successfully loaded {len(input_df)} tasks from {INPUT_CSV_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file not found: {INPUT_CSV_PATH}\")\n",
    "        return\n",
    "\n",
    "    tasks = input_df['task'].tolist()\n",
    "    task_ids = input_df['id'].tolist()\n",
    "\n",
    "    logging.info(f\"Sending {len(tasks)} tasks to LLM for batch processing...\")\n",
    "    math_ir_jsons = llm_generator.generate_batch(tasks)\n",
    "    logging.info(\"LLM batch processing complete.\")\n",
    "\n",
    "    results = []\n",
    "    for i, math_ir_json in enumerate(math_ir_jsons):\n",
    "        task_id = task_ids[i]\n",
    "        logging.info(f\"Processing result for task {task_id}...\")\n",
    "\n",
    "        if not math_ir_json:\n",
    "            logging.warning(f\"Skipping task {task_id} due to LLM generation failure.\")\n",
    "            results.append({'id': task_id, 'answer': 'ERROR_GENERATION'})\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            math_ir = MathIR.model_validate(math_ir_json)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Skipping task {task_id} due to invalid MathIR JSON: {e}\")\n",
    "            results.append({'id': task_id, 'answer': 'ERROR_VALIDATION'})\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            computed_results = run_mathir(math_ir)\n",
    "            answer = next(iter(computed_results.values()), 'NO_ANSWER')\n",
    "            results.append({'id': task_id, 'answer': str(answer)})\n",
    "            logging.info(f\"Task {task_id} processed successfully. Answer: {answer}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Skipping task {task_id} due to a runtime error in mathir_parser: {e}\")\n",
    "            results.append({'id': task_id, 'answer': 'ERROR_COMPUTATION'})\n",
    "\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    logging.info(f\"Output saved to {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
